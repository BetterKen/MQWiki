# 4 ES分词器

## 4.1 Analysis

**文本分析,把全文本转换一系列单词(term/token)的过程,也叫作分词**

## 4.2 Analyzer

**分词通过分词器来实现,可以使用ES内置的分词器,也可以按需定制化分词器**

### 4.2.1 Analyzer组成

![](http://dist415.oss-cn-beijing.aliyuncs.com/esalsis.png)

- Character Filters （针对原始文本处理，例如去除html）

- Tokenizer （按照规则切分为单词）
- Token Filter（将切分的的单词进行加工，小写，删除stopwords,增加同义词）

### 4.2.2 ES内置分词器

- **Standard Analyzer** 一默认分词器，按词切分，小写处理
- **Simple Analyzer** 一按照非字母切分（符号被过滤），小写处理
- **Stop Analyzer** —小写处理，停用词过滤（the, a, is）
- **Whitespace Analyzer** —按照空格切分，不转小写
- **Keyword Analyzer** 一不分词，直接将输入当作输出
- **Patter Analyzer** -正则表达式，默认\W+ （非字符分隔）
- **Language** 一提供了30多种常见语言的分词器
- **Customer Analyzer**自定义分词器

## 4.3 中文分词

常用的中文分词器如下:

- **[IK](https://github.com/medcl/elasticsearch-analysis-ik)**:支持自定义词库，支持热更新分词字典
- **[THULAC](https://github.com/microbun/elasticsearch-thulac-plugin)**:清华大学自然语言处理和社会人文计算实验室的一套中文分词器 



